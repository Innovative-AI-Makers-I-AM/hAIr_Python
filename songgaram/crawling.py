# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import pandas as pd
# import time

# # Selenium WebDriver ì„¤ì •
# options = webdriver.ChromeOptions()
# options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
# driver = webdriver.Chrome()

# # ìŠ¤íƒ€ì¼ ëª©ë¡
# styles = ["ë ˆì´ì–´ë“œì»·", "í—ˆì‰¬ì»·", "ìƒ¤ê¸°ì»·", "ë¦¬í”„ì»·", "ì›ë­ìŠ¤ì»·", "í”½ì‹œì»·", "ë³´ë¸Œì»·", "ë±…í—¤ì–´", "ë¹„ëŒ€ì¹­ì»·", "ë¨¸ì‰¬ë£¸ì»·"]

# # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
# results = []

# # ê° ìŠ¤íƒ€ì¼ì— ëŒ€í•´ í¬ë¡¤ë§
# for style in styles:
#     try:
#         # ì›¹í˜ì´ì§€ URL
#         url = f"https://hairshop.kakao.com/search/style?category=CUT&gender=FEMALE&tags={style}"
#         driver.get(url)
#         time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

#         # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
#         soup = BeautifulSoup(driver.page_source, 'html.parser')
#         first_image = soup.select_one('ul.list_likeStyleImageList img.img_g')
#         image_url = first_image['src'] if first_image else None

#         # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ í´ë¦­
#         first_image_element = driver.find_element(By.CSS_SELECTOR, 'ul.list_likeStyleImageList img.img_g')
#         first_image_element.click()
#         time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

#         # í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ)
#         soup = BeautifulSoup(driver.page_source, 'html.parser')
#         hashtags = soup.select('div.cover_tag a.link_hashtag')
#         hashtag_texts = [tag.text for tag in hashtags[:3]]  # ìµœëŒ€ 3ê°œë§Œ ì¶”ì¶œ

#         results.append({
#             'style': style,
#             'image_url': image_url,
#             'hashtags': ', '.join(hashtag_texts)
#         })
#     except Exception as e:
#         print(f"Error processing style {style}: {e}")

# # WebDriver ì¢…ë£Œ
# driver.quit()

# # ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
# df = pd.DataFrame(results)
# df.to_csv('hairstyles.csv', index=False)

# print("í¬ë¡¤ë§ ì™„ë£Œ ë° CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ.")




# import requests
# from bs4 import BeautifulSoup
# import pandas as pd
# from docx import Document
# import json

# # í¬ë¡¤ë§í•  URL
# urls = ['https://hairshop.kakao.com/magazines/543', 'https://hairshop.kakao.com/magazines/545']

# # ì „ì²´ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
# all_data = []

# # ê° URLì— ëŒ€í•´ í¬ë¡¤ë§ ìˆ˜í–‰
# for i, url in enumerate(urls):
#     # HTTP GET ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŒ
#     response = requests.get(url)
#     response.raise_for_status()  # ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸

#     # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íŒŒì‹±
#     soup = BeautifulSoup(response.text, 'html.parser')

#     # í¬ë¡¤ë§í•  íƒœê·¸ ëª©ë¡
#     tags = ['p', 'h1', 'h3', 'h4', 'h5', 'i', 'span', 'strong']

#     # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
#     data = []

#     # íŠ¹ì • í…ìŠ¤íŠ¸ì™€ íƒœê·¸ë¥¼ ì œì™¸í•˜ê³  HTMLì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
#     for element in soup.find_all(text=True):
#         parent_tag = element.parent.name
#         text = element.strip()
#         if text and parent_tag in tags and text not in ["ğŸ” ì´ ë¨¸ë¦¬ ì–´ë””ì„œ í–ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´? ì‚¬ì§„ì„ ëˆŒëŸ¬ë³´ì„¸ìš”!", "ğŸ‘©ğŸ»â€ğŸ¦±"] and parent_tag != 'h2':
#             data.append(text)

#     # í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
#     full_text = ' '.join(data)
    
#     # ê° í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
#     page_data = {
#         'url': url,
#         'content': full_text
#     }
    
#     # ì „ì²´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
#     all_data.append(page_data)

# # JSON íŒŒì¼ë¡œ ì €ì¥
# with open('crawled_data.json', 'w', encoding='utf-8') as f:
#     json.dump(all_data, f, ensure_ascii=False, indent=4)

# print("í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. JSON íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# # docx íŒŒì¼ í¬ë¡¤ë§ ë¡œì§
# # ê° URLì— ëŒ€í•´ í¬ë¡¤ë§ ìˆ˜í–‰
# for i, url in enumerate(urls):
#     # HTTP GET ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŒ
#     response = requests.get(url)
#     response.raise_for_status()  # ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸

#     # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íŒŒì‹±
#     soup = BeautifulSoup(response.text, 'html.parser')

#     # í¬ë¡¤ë§í•  íƒœê·¸ ëª©ë¡
#     tags = ['p', 'h1', 'h3', 'h4', 'h5', 'i', 'span', 'strong']

#     # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
#     data = []

#     # íŠ¹ì • í…ìŠ¤íŠ¸ì™€ íƒœê·¸ë¥¼ ì œì™¸í•˜ê³  HTMLì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
#     for element in soup.find_all(text=True):
#         parent_tag = element.parent.name
#         text = element.strip()
#         if text and parent_tag in tags and text not in ["ğŸ” ì´ ë¨¸ë¦¬ ì–´ë””ì„œ í–ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´? ì‚¬ì§„ì„ ëˆŒëŸ¬ë³´ì„¸ìš”!", "ğŸ‘©ğŸ»â€ğŸ¦±"] and parent_tag != 'h2':
#             data.append(text)

#     # # í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
#     # full_text = ' '.join(data)
    
#     # Document ê°ì²´ ìƒì„±
#     doc = Document()
#     doc.add_paragraph(data)
    
#     # docx íŒŒì¼ë¡œ ì €ì¥
#     file_name = f'doc_{i+1}.docx'
#     doc.save(file_name)

# print("í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. docx íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


from chromadb import PersistentClient
client = PersistentClient()
collections = client.list_collections()
collection_names = [coll.name for coll in collections]
print(f'1. collection list :  {[collection_names]}')
collection_name = 'test_embeddings'
client.delete_collection(name=collection_name)
print(f'ê¸°ì¡´ ì»¬ë ‰ì…˜ {collection_name}ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.')
collections = client.list_collections()
collection_names = [coll.name for coll in collections]
print(f'2. collection list :  {[collection_names]}')